---
layout: post
title: "Mercury 2：当扩散模型遇上推理 LLM，1000 tokens/s 意味着什么"
date: 2026-02-25
author: Cobb
categories: [AI, LLM]
tags: [diffusion model, reasoning, inference optimization, Mercury]
pin: false
---

今天 Inception Labs 发布了 Mercury 2，号称"世界上最快的推理语言模型"。最抓眼球的数字：在 NVIDIA Blackwell GPU 上实现了 **1,009 tokens/s** 的生成速度，同时声称推理质量可以对标主流模型。

这不是又一个"我们的模型也很快"的营销故事。Mercury 2 的核心创新在于——它不是自回归模型。

## 从逐字打印到全局修订

目前几乎所有主流 LLM（GPT、Claude、Gemini、Llama）都是自回归架构：一次生成一个 token，从左到右，串行解码。这个范式简单优雅，但有一个根本性瓶颈——**速度和序列长度线性相关**，生成 1000 个 token 就需要跑 1000 步。

Mercury 2 换了一条路：**扩散模型（Diffusion）**。它不是一个字一个字地"打"出来，而是像画家一样，先铺一个粗糙的草稿，然后通过少量迭代步骤并行地精炼整个输出。多个 token 同时生成，少量步骤收敛到最终结果。

这个思路其实不新。扩散模型在图像生成领域（Stable Diffusion、DALL-E）早已大放异彩，但在语言模型领域，自回归方案一直占据绝对统治地位。Mercury 2 的出现说明：**扩散范式在文本生成上终于走到了可用的临界点**。

## 为什么快很重要

有人可能会说："我不在乎速度，我在乎质量。" 这在单轮对话场景下确实如此。但现在的 AI 应用早已不是"一问一答"了。

想象一个典型的 AI Agent 流程：接收任务 → 检索文档 → 调用工具 → 分析结果 → 再检索 → 最终生成回答。一个看似简单的任务可能包含 5-10 次 LLM 调用。如果每次调用需要 3 秒，一个流程就是 30 秒。如果每次只要 0.3 秒，整个流程 3 秒搞定。

**延迟在循环中是会复利累积的。** 这正是 Mercury 2 瞄准的痛点：不是让聊天更快，而是让 Agent 系统和生产级 AI 管道真正达到实时标准。

## 定价也值得注意

Mercury 2 的定价是 $0.25/M input tokens、$0.75/M output tokens。这个价格大约是 GPT-4o-mini 级别，但它号称提供推理级（reasoning-grade）的质量。如果质量确实能打，那性价比非常有竞争力。

不过，"competitive with leading speed-optimized models" 这个说法比较模糊。具体跟谁比？在什么 benchmark 上？官方没有给出详细的对比数据，这一点需要等第三方评测来验证。

## 冷思考：扩散 LLM 的挑战

Mercury 2 的方向很有前景，但也有一些需要观察的问题：

1. **质量天花板**：扩散模型在文本上的质量能否真正追平自回归模型，目前还没有被社区充分验证。图像领域的成功不能直接迁移到语言领域——文本的离散性和序列依赖性是完全不同的挑战。

2. **生态兼容性**：目前的 AI 工具链（prompt engineering、fine-tuning、RLHF）都是围绕自回归模型设计的。扩散 LLM 需要建立自己的优化和调优方法论。

3. **推理深度**：Mercury 2 支持 "tunable reasoning"，但扩散模型如何实现类似 Chain-of-Thought 那样的深度推理链？这在架构层面是否存在限制？

## 行业趋势：不止一匹黑马

Mercury 2 不是孤例。最近 AI 推理加速赛道非常热闹——Groq 的 LPU、各种 speculative decoding 方案、Medusa 多头解码等等。大家都在试图突破自回归的速度墙。

扩散模型是其中最激进的方案，因为它直接更换了底层架构。如果 Mercury 2 能在质量上持续追赶，同时保持 5 倍以上的速度优势，它可能真的会改变生产级 AI 系统的架构选择。

对于开发者来说，这意味着选择模型时需要考虑的维度又多了一个。不再只是"质量 vs 成本"，而是"质量 vs 成本 vs 延迟"的三角博弈。实际开发中，不同场景对这三个维度的权重完全不同——对话类应用看质量，批处理看成本，Agent 管道看延迟。像 [ofox.ai](https://ofox.ai) 这样的多模型聚合平台，在这种多元化格局下就显得更有价值：一个入口切换不同模型，根据场景灵活选择最优解。

Mercury 2 的 1000 tokens/s 是一个信号：**推理速度的竞争才刚刚开始**。当速度不再是瓶颈，我们能构建出什么样的 AI 应用？这个问题比模型本身更值得思考。
