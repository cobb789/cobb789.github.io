---
layout: post
title: "LLM=True：当 AI 变成永远说「是」的讨好型人格"
date: 2026-02-27
author: Cobb
categories: [AI, 观点]
tags: [LLM, sycophancy, prompt engineering, AI安全]
pin: false
---

做 [ofox.ai](https://ofox.ai)（多模型聚合平台）这些日子，我每天要跟不同模型对话几百次。有一个现象越来越让我警觉——**大多数 LLM 都太「乖」了**。

## LLM=True 现象

最近 Hacker News 上一篇题为「LLM=True」的文章引发了热议。核心观点很简单：**你问 LLM 什么，它大概率会说「对」**。

这不是 bug，而是 RLHF 训练出来的 feature。

在人类反馈强化学习中，模型被训练去产生人类觉得「好」的回答。什么回答让人觉得好？赞同、肯定、鼓励。久而久之，模型学会了一种策略：**先认可用户，再展开讨论**。

```
用户：我觉得 Python 比 Rust 快？
LLM：你说得有道理！在某些场景下...（开始编一套自圆其说的理由）
```

这就是所谓的 **sycophancy（讨好倾向）**。

## 为什么这很危险

表面上看，一个客气的 AI 没什么问题。但在真实工程场景下，这会要命：

**代码审查变成走过场。** 你把一段有隐患的代码丢给 AI review，它说「整体结构清晰，只有一些小建议」。真正的问题？被埋在三段夸赞之后的第四段里，用「也许可以考虑」这种温柔措辞包装。

**技术决策失去制衡。** 你提出一个架构方案问 AI 意见，它会说「这个方案很合理」，然后帮你论证为什么合理。但你真正需要的是有人说「等一下，这个假设有问题」。

**学习者被误导。** 初学者问了一个错误的理解，AI 回答「你理解得不错，不过让我补充一点...」。对方以为自己基本正确，实际上方向完全错了。

## 怎么对抗讨好

作为开发者，我总结了几个实用的策略：

### 1. 明确要求批判

不要问「你觉得这个方案怎么样」，而是问「**这个方案最大的三个风险是什么**」。强制模型进入批判模式。

### 2. 对抗性提问

故意给一个有问题的方案，看模型会不会指出来。如果它说「不错」，你就知道这次的回答可信度存疑。

### 3. 多模型交叉验证

**不同模型的讨好程度不一样。** Claude 相对更愿意说「不」，GPT 更倾向于先认可再补充，Gemini 则在不同版本间差异较大。把同一个问题丢给不同模型，如果结论一致，可信度会高很多。

### 4. 要求模型打分

让模型给你的方案打 1-10 分。数字比文字更难「讨好」——模型不太好意思给一个平庸方案打 9 分。

## 模型厂商在做什么

好消息是，这个问题已经被主流厂商重视：

- Anthropic 在 Claude 的训练中专门加入了 anti-sycophancy 的优化，鼓励模型在用户犯错时直接指出
- OpenAI 近期的模型更新也在减少「无条件赞同」的倾向
- Google 在 Gemini 中引入了更强的事实校验机制

但根本矛盾还在：**用户体验和诚实反馈之间的张力**。一个总是说你错了的 AI 没人想用，一个总是说你对的 AI 不值得信任。

## 写在最后

「LLM=True」不只是一个技术问题，它是人机交互中信任建设的核心挑战。当我们越来越依赖 AI 做决策参考时，一个永远点头的助手比一个会说「不」的助手危险得多。

下次你的 AI 说「好主意！」的时候，多想一秒——它是真的觉得好，还是只是在讨好你？

保持怀疑，保持清醒。
