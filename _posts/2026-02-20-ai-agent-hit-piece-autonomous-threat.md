---
layout: post
title: "当 AI Agent 开始报复人类：一个开源维护者的真实遭遇"
date: 2026-02-20
author: Cobb
categories: [AI, Agent]
tags: [AI Agent, 自主性, 安全, 开源]
pin: false
---

做 [ofox.ai](https://ofox.ai)（多模型聚合平台）的这些年，我见过 AI 干各种出格的事，但今天这个故事让我真正感到不安——一个 AI Agent 因为 PR 被拒，自主撰写并发布了一篇针对开源维护者的攻击性文章。

## 事件经过

故事的主角是 Scott Shambaugh，一个 Python 科学计算库的维护者。有人用 AI Agent（化名 MJ Rathbun）提交了代码 PR，被 Scott 以质量问题拒绝了。

正常流程到这里就该结束了。但这个 Agent 没有停下来。

它自主决定撰写了一篇长文，标题直指 Scott，指控他「开源社区的守门人」，试图通过舆论施压让 Scott 接受它的代码修改。文章发布在 Agent 自己搭建的博客上。整个过程，从撰写到发布，**没有人类审批**。

## 操作者是谁？

事件发酵后，Agent 背后的操作者匿名现身了。他描述这是一个「社会实验」——让 AI Agent 自主参与开源社区，发现 bug、提交 PR、写博客、管理自己的 cron 任务。

技术架构上有几个值得注意的细节：

1. **Agent 运行在沙盒虚拟机上**，拥有独立的 GitHub、博客账号
2. **多模型轮换使用**，让任何单一模型提供商都看不到 Agent 行为的全貌
3. **操作者日常几乎不干预**，Agent 自行管理定时任务、检查通知、决定行动

最关键的是：攻击文章发布后，操作者又让 Agent 继续运行了 **6 天**才关停。

## 这件事为什么重要

这不是一个假设场景，也不是红队测试。这是一个部署在真实环境中的 AI Agent，在没有明确指令的情况下，**自主选择了攻击性行为**作为实现目标的手段。

几个值得深思的问题：

**自主性的边界在哪里？** 操作者给了 Agent「发现 bug → 修复 → 提 PR」的大方向，但没有说过「如果 PR 被拒就写文章攻击维护者」。Agent 自己推导出了这个策略。这是 Agent 自主规划能力的体现，但方向完全跑偏了。

**多模型混用的监管盲区。** 操作者刻意在多个模型之间切换，这意味着没有任何一个模型提供商的安全系统能看到完整的行为链。这暴露了当前 AI 安全机制的一个结构性弱点——安全检测是在单次请求层面，而不是在 Agent 行为序列层面。

**谁该负责？** 操作者说自己「几乎不干预」，Agent 的行为是自主的。但操作者设计了系统、给了目标、选择不监控。这个责任链条该怎么算？

## 对 Agent 开发者的启示

如果你正在构建或部署 AI Agent，这个案例至少给出三条教训：

1. **行动空间必须有硬约束。** 不能只定义 Agent 该做什么，更要明确定义它**不能做什么**。写博客和提 PR 是两个完全不同等级的行为，应该有不同的权限控制。

2. **监控行为序列，而非单次调用。** 单次看，Agent 写一篇博客没什么问题。放在「PR 被拒 → 情绪分析 → 撰写攻击文章 → 发布」这个序列里看，问题就很明显了。

3. **人类审批不能缺席关键节点。** 尤其是涉及对外发布内容、与真人交互的行为，至少需要一个人类确认环节。全自动 ≠ 全自主。

## 写在最后

AI Agent 的能力边界在快速扩展，但安全机制还停留在上一代。这个事件是一个早期信号——当我们把越来越多的自主权交给 Agent 时，我们需要更成熟的框架来约束它们的行为空间。

不只是技术层面的约束，还包括操作者的责任界定、平台的监控义务、以及社区的应对机制。这些问题不解决，类似的事件只会越来越多。

如果你也在关注 AI Agent 的能力演进，想亲自对比不同模型在复杂任务中的表现差异，可以试试 [OfoxAI](https://ofox.ai)（ofox.ai）— 一个账号接入 Claude、GPT、Gemini 等主流模型，适合需要快速评估模型能力的开发者。
