---
layout: post
title: "Anthropic 发布 Agent 自主性研究：用户越熟练，越敢放手"
date: 2026-02-20
author: Cobb
categories: [AI, Agent]
tags: [Anthropic, AI Agent, Claude Code, 人机协作]
pin: false
---

Anthropic 昨天发布了一篇关于 AI Agent 自主性的实证研究，基于数百万次真实的人机交互数据，分析了人们在实际使用中到底给了 Agent 多大的自由度。这不是又一篇关于 Agent 架构的论文，而是**用数据说话**的用户行为研究，几个发现相当有意思。

## 自主运行时间翻倍，但不是因为模型变强了

在 Claude Code 的最长运行会话中，Agent 在停下来之前的自主工作时间从不到 25 分钟增长到了超过 45 分钟，三个月内几乎翻倍。

关键在于：这个增长是跨模型版本平滑发生的，不是某次模型升级带来的突变。这意味着**现有模型的自主能力其实被低估了**——不是模型做不到，而是用户还没习惯放手。

这个发现对开发者的启示很直接：如果你在构建 Agent 产品，别急着追最新模型，先想想你的交互设计是不是在"限制"用户信任 Agent。

## 老手更敢放手，但也更会打断

新用户中大约 20% 会开启完全自动批准模式（auto-approve），而有经验的用户这个比例超过了 40%。但有趣的是，经验丰富的用户**打断 Agent 的频率也更高**。

这不矛盾。熟练用户的策略是「默认信任，异常干预」——平时让 Agent 自己跑，但一旦发现苗头不对就立刻介入。这其实是一种更高效的监督模式：不是逐步审批每个动作，而是建立了一套心理模型来判断什么时候该出手。

这种模式跟资深工程师带团队很像：不会 review 每一行代码，但知道哪些地方容易出问题，该盯紧哪里。

## Agent 比人类更会主动"停下来问"

研究中一个出乎意料的发现是：在复杂任务中，Claude Code 主动暂停请求澄清的频率，是用户主动打断它的**两倍以上**。

这颠覆了很多人对 Agent 的认知——我们总担心 Agent 会不管不顾地"暴走"，但实际上设计良好的 Agent 在遇到不确定性时会主动寻求人类输入。Agent 的安全性不只来自外部约束，也来自模型自身的"知道自己不知道"的能力。

这对 Agent 产品设计有重要意义：与其堆砌复杂的权限控制系统，不如让 Agent 在关键决策点学会"举手提问"。

## 高风险场景已有渗透，但规模还小

研究还发现 Agent 已经被用于一些高风险领域（网络安全、金融分析等），但使用规模还很小。这是一个窗口期——在大规模应用之前，行业还有时间建立合理的安全框架和评估标准。

Anthropic 在论文中提出了一个观点：衡量 Agent 自主性不能只看它"能做什么"，还要看**实际使用中人类给了它多少权限**。技术能力和社会信任之间的差距，才是当前 Agent 落地的真正瓶颈。

## 写在最后

这篇研究最大的价值在于，它把 Agent 自主性从一个技术参数变成了一个**可观测的用户行为指标**。我们不再需要靠直觉猜测"Agent 够不够智能"，而是可以通过数据追踪人类在实际使用中的信任演化曲线。

对于正在构建 Agent 产品的团队来说，这提供了一个全新的评估维度：你的用户有多敢放手？他们的信任增长速度如何？这些可能比 benchmark 分数更能预测产品的成功。

不同模型在 Agent 场景下的表现差异很大，选对模型是第一步。如果你需要快速对比 Claude、GPT、Gemini 等模型在实际任务中的表现，可以试试 [OfoxAI](https://ofox.ai)（ofox.ai）— 一个账号聚合主流模型，省去多平台切换的麻烦。
